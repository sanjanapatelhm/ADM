#!/usr/bin/env python
# coding: utf-8

# # Visual Search Streamlit code

# In[ ]:


get_ipython().run_cell_magic('writefile', 'app.py', '\nimport tensorflow as tf\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nfrom tensorflow.keras.models import Model\nprint(tf.__version__)\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\'pdf.fonttype\': \'truetype\'})\nfrom matplotlib import offsetbox\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn import manifold\nimport scipy as sc\n\nimport streamlit as st\nimport numpy as np\nfrom tqdm import tqdm\n\nimport glob\nimport ntpath\nimport cv2\nfrom PIL import Image\n\n\nimage_paths = glob.glob(\'img/1981_Graphic_Ringer_Tee/*.jpg\')\nimages = {}\nfor image_path in image_paths:\n    image = cv2.imread(image_path, 3)\n    g, b, r = cv2.split(image) # get b, g, r\n    image = cv2.merge([r,g,b]) # switch it to r, g, b\n    image = cv2.resize(image, (200, 200))\n    images[ntpath.basename(image_path)] = image\nn_col = 8\nn_row = int(len(images)/n_col)\nf, ax = plt.subplots(n_row, n_col, figsize=(16, 8))\nfor i in range(n_row):\n    for j in range(n_col):\n        ax[i, j].imshow(list(images.values())[n_col*i + j])\n        ax[i, j].set_axis_off()\n\n\n\n\n\ndef load_image(image):\n    image = plt.imread(image)\n    img = tf.image.convert_image_dtype(image, tf.float32)\n    img = tf.image.resize(img, [400, 400])\n    img = img[tf.newaxis, :] # shape -> (batch_size, h, w, d)\n    return img\n\n#\n# content layers describe the image subject\n#\ncontent_layers = [\'block5_conv2\'] \n\n#\n# style layers describe the image style\n# we exclude the upper level layes to focus on small-size style details\n#\nstyle_layers = [ \n        \'block1_conv1\',\n        \'block2_conv1\',\n        \'block3_conv1\', \n        #\'block4_conv1\', \n        #\'block5_conv1\'\n    ] \n\ndef selected_layers_model(layer_names, baseline_model):\n    outputs = [baseline_model.get_layer(name).output for name in layer_names]\n    model = Model([vgg.input], outputs)\n    return model\n\n# style embedding is computed as concatenation of gram matrices of the style layers\ndef gram_matrix(input_tensor):\n    result = tf.linalg.einsum(\'bijc,bijd->bcd\', input_tensor, input_tensor)\n    input_shape = tf.shape(input_tensor)\n    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n    return result/(num_locations)\n\nclass StyleModel(tf.keras.models.Model):\n    def __init__(self, style_layers, content_layers):\n        super(StyleModel, self).__init__()\n        self.vgg =  selected_layers_model(style_layers + content_layers, vgg)\n        self.style_layers = style_layers\n        self.content_layers = content_layers\n        self.num_style_layers = len(style_layers)\n        self.vgg.trainable = False\n\n    def call(self, inputs):\n        # scale back the pixel values\n        inputs = inputs*255.0\n        # preprocess them with respect to VGG19 stats\n        preprocessed_input = preprocess_input(inputs)\n        # pass through the reduced network\n        outputs = self.vgg(preprocessed_input)\n        # segregate the style and content representations\n        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n                                          outputs[self.num_style_layers:])\n\n        # calculate the gram matrix for each layer\n        style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n\n        # assign the content representation and gram matrix in\n        # a layer by layer fashion in dicts\n        content_dict = {content_name:value\n                    for content_name, value\n                    in zip(self.content_layers, content_outputs)}\n\n        style_dict = {style_name:value\n                  for style_name, value\n                  in zip(self.style_layers, style_outputs)}\n\n        return {\'content\':content_dict, \'style\':style_dict}\n\nvgg = tf.keras.applications.VGG19(include_top=False, weights=\'imagenet\')\n\ndef image_to_style(image_tensor):\n    extractor = StyleModel(style_layers, content_layers)\n    return extractor(image_tensor)[\'style\']\n\ndef style_to_vec(style):\n    # concatenate gram matrics in a flat vector\n    return np.hstack([np.ravel(s) for s in style.values()]) \n\n#\n# Print shapes of the style layers and embeddings\n#\nimage_tensor = load_image(image_paths[0])\nstyle_tensors = image_to_style(image_tensor)\nfor k,v in style_tensors.items():\n    print(f\'Style tensor {k}: {v.shape}\')\nstyle_embedding = style_to_vec( style_tensors )\nprint(f\'Style embedding: {style_embedding.shape}\')\n\n#\n# compute styles\n#\nimage_style_embeddings = {}\nfor image in tqdm(image_paths): \n    image_tensor = load_image(image)\n    style = style_to_vec( image_to_style(image_tensor) )\n    image_style_embeddings[ntpath.basename(image)] = style\n#\n# Visualize the 2D-projection of the embedding space with example images (thumbnails)\n#\ndef embedding_plot(X, images, thumbnail_sparsity = 0.005, thumbnail_size = 0.3):\n    x_min, x_max = np.min(X, axis=0), np.max(X, axis=0)\n    X = (X - x_min) / (x_max - x_min)\n    fig, ax = plt.subplots(1, figsize=(12, 12))\n\n    shown_images = np.array([[1., 1.]])\n    for i in range(X.shape[0]):\n        if np.min(np.sum((X[i] - shown_images) ** 2, axis=1)) < thumbnail_sparsity: continue\n        shown_images = np.r_[shown_images, [X[i]]]\n        thumbnail = offsetbox.OffsetImage(images[i], cmap=plt.cm.gray_r, zoom=thumbnail_size)\n        ax.add_artist(offsetbox.AnnotationBbox(thumbnail, X[i], bboxprops = dict(edgecolor=\'white\'), pad=0.0))\n\n    plt.grid(True)\n    \ntsne = manifold.TSNE(n_components=2, init=\'pca\', perplexity=10, random_state=0)\nX_tsne = tsne.fit_transform( np.array(list(image_style_embeddings.values())) )\nembedding_plot(X_tsne, images=list(images.values()))\n\ndef search_by_style(image_style_embeddings, images, reference_image, max_results=10):\n    v0 = image_style_embeddings[reference_image]\n    distances = {}\n    for k,v in image_style_embeddings.items():\n        d = sc.spatial.distance.cosine(v0, v)\n        distances[k] = d\n\n    sorted_neighbors = sorted(distances.items(), key=lambda x: x[1], reverse=False)\n    \n    f, ax = plt.subplots(1, max_results, figsize=(16, 8))\n    final_result=[]\n    for i, img in enumerate(sorted_neighbors[:max_results]):\n        final_result.append(images[img[0]])\n    st.image(final_result)\n        \n#         ax[i].imshow(images[img[0]])\n#         ax[i].set_axis_off()\n\ndef show_result_images(image):\n    search_by_style(image_style_embeddings,images,image)\n    pass\n\nst.title(\'Visual Search Web Application\')\n\nuploaded_file = st.file_uploader("Choose a image file")\nif uploaded_file is not None:\n    image = uploaded_file.read()\n    img = st.image(image, caption=\'Sample Image\', use_column_width=True)\n\nbutton = st.button("Search")\nif button :\n    show_result_images(uploaded_file.name)')


# In[ ]:


get_ipython().system('streamlit run app.py')


# In[ ]:




